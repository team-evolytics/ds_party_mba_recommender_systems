{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a6aa4c",
   "metadata": {},
   "source": [
    "# Model Input and Output Examples\n",
    "\n",
    "This notebook is intended to demonstrate and clarify the inputs and outputs for LightFM recommender models. I've modified some of code from examples in the lightfm documentation to do this. We'll start by installing lightfm, if not previously installed, importing a demo dataset, and then demonstrating inputs and outputs for prediction after fitting a model. \n",
    "\n",
    "Please note that this code is for demonstration only - given it's purpose there are no efforts made to evaluate models.  It is not polished code and is only intended to be illustrative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only if lightfm is not installed. \n",
    "!conda install -c conda-forge lightfm -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d87b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries.  \n",
    "import numpy as np\n",
    "from lightfm import LightFM\n",
    "from lightfm import evaluation\n",
    "from lightfm.datasets import fetch_stackexchange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8dd487",
   "metadata": {},
   "source": [
    "## Stackoverflow Demo Dataset\n",
    "\n",
    "If you're curious about the dataset you can find [more information here.](https://making.lyst.com/lightfm/docs/_modules/lightfm/datasets/stackexchange.html) The most important thing to note is that train and test dataset use the same items and features (i.e., they are matrices w/ the same shape).  The difference between them is that some of the user-item interactions are held out for evaluating the accuracy of the recommender system.  \n",
    "\n",
    "The item features here are tags applied to stackoverflow questions.  In Pushly's case, the item features would have typically been a score or value for each vocabulary term used in the corpus (i.e., the body of documents we are making predictions about). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f499e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 3213 users and 72360 items, with 4453 interactions in the test and 57795 interactions in the training set.\n",
      "\n",
      "There are 1246 distinct tags, with values like ['bayesian', 'prior', 'elicitation'].\n"
     ]
    }
   ],
   "source": [
    "# Getting example dataset. \n",
    "data = fetch_stackexchange('crossvalidated',\n",
    "                           test_set_fraction=0.1,\n",
    "                           indicator_features=False,\n",
    "                           tag_features=True)\n",
    "\n",
    "train = data['train']\n",
    "test = data['test']\n",
    "\n",
    "print('The dataset has %s users and %s items, '\n",
    "      'with %s interactions in the test and %s interactions in the training set.\\n'\n",
    "      % (train.shape[0], train.shape[1], test.getnnz(), train.getnnz()))\n",
    "\n",
    "item_features = data['item_features']\n",
    "tag_labels = data['item_feature_labels']\n",
    "\n",
    "print('There are %s distinct tags, with values like %s.' % (item_features.shape[1], tag_labels[:3].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d5a19",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Here we are actually training our model.  Note that we pass in our training dataset (i.e., the matrix of user-item interactions) and the item features.  It's important to note that if you train the model with item features you have to use the item features in prediction.  Remember that the training and test dataset only differ on the basis of the interactions (i.e., the test dataset has additional interactions) but have the same users and items.  As a result, the item features used to train the model are appropriate to pass when we engage in prediction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3605fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 309 ms, sys: 1.93 ms, total: 311 ms\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "# Set the number of threads; you can increase this\n",
    "# if you have more physical cores available.\n",
    "NUM_THREADS = 2\n",
    "NUM_COMPONENTS = 30\n",
    "NUM_EPOCHS = 3\n",
    "ITEM_ALPHA = 1e-6\n",
    "\n",
    "# Define a new model instance\n",
    "model = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                no_components=NUM_COMPONENTS)\n",
    "\n",
    "# Fit the hybrid model. Note that we pass in the item features matrix\n",
    "# w/ the training data of user/item interactions. .\n",
    "%time model = model.fit(train, item_features=item_features, \\\n",
    "                        epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f871670",
   "metadata": {},
   "source": [
    "## Making Predictions \n",
    "\n",
    "When making a prediction, you should use the model.predict method.  This method takes two 1 dimensional numpy arrays, user and items, that are of equal length. The user array consists of the user ids (i.e., indices in the matrix) for the user for whom we wish to make predictions.  The item array has the item ids (i.e., indices in the matrix) for items for which we want to predict a rating. In real world application, we would create mappings of indices to actual user and item ids (e.g., a python dictionary of index:id) in order to link predictions back to the users and items. \n",
    "\n",
    "Importantly, when making predictions we need to repeat the user id for every item we wish to predict and the set of item ids should be repeated for each user. For example, if we wanted to predict the ratings for users 4, 6, 8 on items 3, 12, and 48 the arrays that would be passed would be: \n",
    "\n",
    "- user = [4, 4, 4, 6, 6, 6, 8, 8, 8]\n",
    "\n",
    "- item = [3, 12, 48, 3, 12, 48, 3, 12, 48]\n",
    "\n",
    "I've written a function below to show how this can be done programmatically and give you some idea of the output.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c605f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_demo(model, u, i=None, mtrx=None):\n",
    "    \"\"\"Generates predictions using lightfm model. \n",
    "    \n",
    "    Function is designed to demonstrate how multiple input types can be \n",
    "    manipulated efficiently into the required format and outputs predictions\n",
    "    for inspection.  Also returns user ids to enable predictions.   \n",
    "    \n",
    "    Args:\n",
    "        u (np array): Array of indices for users for whom we want to make \n",
    "                      predictions. \n",
    "        i (np array): Array of indices for items we wish to recommend. \n",
    "        mtrx (np 2D array): user x items matrix (e.g. test or train matrix)\n",
    "    \n",
    "    Returns:\n",
    "        np array: Array of users indices corresponding to predictions\n",
    "        np array: Array of item indices corresponding to predictions. \n",
    "        np array: Array of prediction scores \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if mtrx is not None: \n",
    "        _, n_items = mtrx.shape\n",
    "        items = np.tile(np.arange(n_items), len(u))\n",
    "    elif i is not None: \n",
    "        n_items = len(i)\n",
    "        items = np.tile(i, len(u)) \n",
    "    else: \n",
    "        print(\"No items or matrix supplied.\")\n",
    "        return \n",
    "    \n",
    "    users = np.repeat(u, n_items)\n",
    "    \n",
    "    assert len(users) == len(items), \"Users and items are different lengths.\"\n",
    "    \n",
    "    predictions = model.predict(users, \n",
    "                                items, \n",
    "                                item_features=item_features)\n",
    "\n",
    "    return users, items, predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da848a8",
   "metadata": {},
   "source": [
    "#### Making Predictions for a Single User \n",
    "\n",
    "Let's make a prediction for a single user using the function above.  It will return an array of prediction scores.  To figure out which items to recommend we simply take the top N values from the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63d45745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    4.             0.            -1.93492508]\n",
      " [    4.             1.            -1.12955868]\n",
      " [    4.             2.            -0.95335263]\n",
      " ...\n",
      " [    4.         72357.            -2.59813523]\n",
      " [    4.         72358.            -2.08621049]\n",
      " [    4.         72359.            -2.92063475]]\n",
      "\n",
      "Number of users: 1\n",
      "Number of items: 72360\n",
      "Users X items: 72360\n",
      "\n",
      "Number of predictions calculated: 72360\n",
      "\n",
      "For user 4 the top 5 predictions would be: [71944 27431  9068 10534  4045]\n"
     ]
    }
   ],
   "source": [
    "# Number of recommendations we would like. \n",
    "top_n = 5\n",
    "\n",
    "np.set_printoptions(suppress=True) # suppressing scientific notation in output\n",
    "\n",
    "users = [4]\n",
    "user_id, item_id, predictions = pred_demo(model, users, mtrx=data['train'])\n",
    "print(np.column_stack((user_id, item_id, predictions)))\n",
    "\n",
    "\n",
    "print(\"\\nNumber of users: {}\".format(len(users)))\n",
    "_, n_items = data['train'].shape\n",
    "print(\"Number of items: {}\".format(n_items))\n",
    "print(\"Users X items: {}\".format(len(users)*n_items))\n",
    "print(\"\\nNumber of predictions calculated: {}\".format(len(predictions)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get indices for top predictions.  \n",
    "indices = np.argpartition(predictions, -top_n)[-top_n:]\n",
    "\n",
    "print (f\"\\nFor user {users[0]} the top {top_n} predictions would be: {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036a2ed",
   "metadata": {},
   "source": [
    "#### Predictions for a subset of Users and Items \n",
    "\n",
    "Remember that when calling model.predict that the items need to have the format:  \n",
    "\n",
    "- user = [4, 4, 4, 6, 6, 6, 8, 8, 8] \n",
    "- item = [3, 12, 48, 3, 12, 48, 3, 12, 48]\n",
    "\n",
    "By using numpy's built-in methods np.repeat and np.tile, respectively, we can pass simple lists of single indices to the above function: \n",
    "\n",
    "- user = [4, 6, 8] \n",
    "- item = [3, 12, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe38bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          3.         -1.79501843]\n",
      " [ 4.         12.         -1.53563535]\n",
      " [ 4.         48.         -2.71290112]\n",
      " [ 6.          3.         -0.60396689]\n",
      " [ 6.         12.         -0.38265514]\n",
      " [ 6.         48.         -1.86493695]\n",
      " [ 8.          3.         -1.58035123]\n",
      " [ 8.         12.         -5.30431223]\n",
      " [ 8.         48.          0.47020158]]\n",
      "\n",
      "Number of users: 3\n",
      "Number of items: 3\n",
      "Users X items: 9\n",
      "\n",
      "Number of predictions calculated: 9\n"
     ]
    }
   ],
   "source": [
    "users = [4, 6, 8]\n",
    "items = [3, 12, 48]\n",
    "\n",
    "user_id, item_id, predictions = pred_demo(model, users, items)\n",
    "\n",
    "print(np.column_stack((user_id, item_id, predictions)))\n",
    "print(\"\\nNumber of users: {}\".format(len(users)))\n",
    "print(\"Number of items: {}\".format(len(items)))\n",
    "print(\"Users X items: {}\".format(len(users)*len(items)))\n",
    "print(\"\\nNumber of predictions calculated: {}\".format(len(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c52351",
   "metadata": {},
   "source": [
    "#### Using Matrix Shape to Make Predictions for all Items\n",
    "\n",
    "If we pass the above function the training or test matrix it will calculate the predictions for all items for the given users. Note that it uses the matrix shape - not the content of the matrix so in this case we can pass it either the test or training matrix as they have the same shape.  In this example, I've implemented it to use all items if a matrix is provided but you could also code it to use all users as well or make the use of the matrix shape contingent upon user and item arrays being unprovided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fbc0f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    4.             0.            -1.93492508]\n",
      " [    4.             1.            -1.12955868]\n",
      " [    4.             2.            -0.95335263]\n",
      " ...\n",
      " [    8.         72357.            -2.22958422]\n",
      " [    8.         72358.             0.36491704]\n",
      " [    8.         72359.             0.34049028]]\n",
      "\n",
      "Number of users: 3\n",
      "Number of items: 72360\n",
      "Users X items: 217080\n",
      "\n",
      "Number of predictions calculated: 217080\n"
     ]
    }
   ],
   "source": [
    "users = [4, 6, 8]\n",
    "user_id, item_id, predictions = pred_demo(model, users, mtrx=data['train'])\n",
    "print(np.column_stack((user_id, item_id, predictions)))\n",
    "\n",
    "\n",
    "print(\"\\nNumber of users: {}\".format(len(users)))\n",
    "_, n_items = data['train'].shape\n",
    "print(\"Number of items: {}\".format(n_items))\n",
    "print(\"Users X items: {}\".format(len(users)*n_items))\n",
    "print(\"\\nNumber of predictions calculated: {}\".format(len(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009fdf75",
   "metadata": {},
   "source": [
    "#### Predictions for a Single Item\n",
    "\n",
    "We can input a single article and get an output for all users as long as the user array contains all user indices!  This tells us which users shoudl be most  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23480f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.            1.           -1.27421498]\n",
      " [   1.            1.           -1.76793253]\n",
      " [   2.            1.           -0.74393654]\n",
      " ...\n",
      " [3210.            1.           -0.19669344]\n",
      " [3211.            1.           -0.04075264]\n",
      " [3212.            1.            0.31518275]]\n",
      "\n",
      "Number of users: 3213\n",
      "Number of predictions calculated: 3213\n"
     ]
    }
   ],
   "source": [
    "# All users(3,216) in data set x all items (72,360)\n",
    "n_users,_ = data['test'].shape \n",
    "users = np.arange(n_users)\n",
    "\n",
    "item = [1] # we only want predictions for item at index position 1.\n",
    "\n",
    "user_id, item_id, predictions = pred_demo(model, users, item)\n",
    "print(np.column_stack((user_id, item_id, predictions)))\n",
    "print(\"\\nNumber of users: {}\".format(n_users))\n",
    "print(\"Number of predictions calculated: {}\".format(len(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e902bd",
   "metadata": {},
   "source": [
    "#### Predicting Ratings for All Items for All Users\n",
    "\n",
    "One question you might ask is how to input to the model/prediction was all articles and the output is for all users (i.e., all items x all users).  You can do that but in the real world Pushly would likely have not as there is not point recommending items that the user had already seen and you would not want to recommend very old articles. \n",
    "\n",
    "I decided to see how lightfm performed predicting all items in the dataset for all users to stress test the prediction method with the data I've got on hand.  I make no promises regarding how this scales but considering the Pushly likely only want to predict three or four weeks of items (i.e., back of napkin if you have 80 articles per day X 28 days ~2240 items) this is encouraging.  You will have a much larger userbase but a smaller number of items that need prediction than what is represented here (approx. 72,000). On my laptop locally, you can do about 230 million predictions in about 20 seconds.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28144b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 5.15 s, total: 25.6 s\n",
      "Wall time: 26.4 s\n",
      "\n",
      "Number of predictions calculated: 232492680\n"
     ]
    }
   ],
   "source": [
    "# All users(3,216) in data set x all items (72,360)\n",
    "n_users,_ = data['test'].shape \n",
    "users = np.arange(n_users)\n",
    "\n",
    "%time user_id, item_id, predictions = pred_demo(model, users, mtrx=data['train'])\n",
    "\n",
    "print(\"\\nNumber of predictions calculated: {}\".format(len(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856900b",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "As you can see it's relatively easy to manipulate a list of indices in the correct format and if you are predicting more than a single item for more than one user you can easily massage it into a three column data structure (i.e., user id, item id, and prediction score). \n",
    "\n",
    "Want to learn more?  Microsoft has an indepth guide to a dozens of recommender models including LightFM!: https://github.com/microsoft/recommenders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
